---
title: "8 Tree-Based Models"
author: "Andrew Liang"
date: "11/22/2020"
output: pdf_document
---
# Notes

* Useful for interpretation, but typically not competitive with best supervised learning approaches in terms of prediction

## Basics of Decision Trees

### Regression Trees

**Process of Building a Regression Tree:**

1. Divide the predictor space (set of possible values of $X_{1},X_{2},\ldots,X_{p}$ into $J$ distinct and non-overlapping regions $R_{1},R_{2},\ldots,R_{J}$

2. For every observation in the$R_{j}$ region, we make the same prediction, which is simply the mean of the response values for the training observations in $R_{j}$

* In step 1, we construct $R_{1},R_{2},\ldots,R_{J}$ such that the predictor space is divided into high-dimensional rectangles, or _boxes_, for ease of interpretation
  * goal is to minimize RSS given by:
  
$$ \sum_{j=1}^J \sum_{i \in R_{j}}(y_{i}-\hat{y}_{R_{j}})^2$$

where $\hat{y}_{R_{j}}$ is the mean response for the training observations within the $j$th box.

* Another way to look at it is that we consider all predictors $X_{1},X_{2},\ldots,X_{p}$ and all possible values of cutpoint $s$ for each of the predictors, then choose the predictor and cutpoint such that the resulting tree has lowest RSS
  * for any $j$ and $s$, we define pair of half-planes:

$$R_{1}(j,s)=\{X|X_{j}<s\} \hspace{0.5cm} \text and \hspace{0.5cm} R_{2}(j,s)=\{X|X_{j} \ge s\}$$

and we seek to value of $j$ and $s$ to minimize:

$$\sum_{i: x_{i} \in R_{1}(j,s)}(y_{i}-\hat{y}_{R_{1}})^2 + \sum_{i: x_{i} \in R_{2}(j,s)}(y_{i}-\hat{y}_{R_{2}})^2$$

where $\hat{y}_{R_{1}}$ is the mean response for training observations in $R_{1}(j,s)$ and $\hat{y}_{R_{2}}$ is mean response for train obs in $R_{2}(j,s)$

Once the regions $R_{1},R_{2},\ldots,R_{J}$ have been created, we can then predict response for a given test observation using mean of train obs in the region to which that test obs belongs

#### Tree Pruning

* Process above may likely overfit data as the resulting tree may be too complex
  * smaller tree with fewer splits can lead to lower variance and better interpretation at the cost of a little bias
* Better strategy may be to grow a very large tree $T_{0}$, and then _prune_ it back to get a subtree
  * want to get a subtree that leads us to the lowest test error rate
  * however, using CV for every subtree may be infeasible, so we need to select a small set of subtrees
* can use _cost complexity pruning_, consider a sequence of trees indexed by a nonnegative tuning parameter $\alpha$
  * for each value of $\alpha$, there corresponds a subtree $T \subset T_{0}$ such that it minimizes:
  
$$ \sum_{m=1}^{|T|} \sum_{i: x \in R_{m}} (y_{i}-\hat{y}_{R_{m}})^2 + \alpha|T|$$
where $|T|$ indicates the number of terminal nodes of tree $T$, and $R_{m}$ is the region corresponding to the $m$th terminal node

* as the number of terminal nodes increases, there is a penalty $\alpha$, so the above quantity will tend to be minimzied for a smaller subtree
  * select $\alpha$ using CV
  
**Algorithm for building Regression Trees**

1. Use recursive binary splitting to grow a large tree on training data, stopping when each terminal node has fewer than some minimum # of observations

2. Apply cost complexity pruning to large tree in order to obtain a sequence of best subtrees as a function of $\alpha$

3. Use K-fold CV to choose $\alpha$. Divide training observations into $K$ fold. For each $k=1,\ldots,K$:
    * repeat steps 1 and 2 on all but $k$th fold of training data
    * evaluate mean squared prediction error on data in left-out $k$th fold, as a function of $\alpha$
    * average out the results for each value of $\alpha$, and pick $\alpha$ to minimize error
  
4. Return the subtree from step 2 that corresponds to chosen value of $\alpha$

### Classification Trees

* very similar to regression tree, but predicts qualitative response instead
  * predict that each observation belongs to the _most commonly occuring class_ or training observations in the region to which it belongs
* also interested in the _class proportions_ among training observations that fall into that region
* also use recursive binary splitting to grow a classification tree, but instead of RSS, _classification error rate_ is used as the criterion for making the binary splits
  * simply the fraction of training obs in that region that don't belong to the most common class:

$$E = 1-\max_{k}(\hat{p}_{mk})$$
where $\hat{p}_{mk}$ is the proportion of training observations in th $m$th region that are from the $k$th class. In practice however, two other measures are preferable:

* The _Gini index_:

$$G = \sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})$$
is a measure of total variance (of which we want to minimize) across the $K$ classes. G takes on a small value if $\hat{p}_{mk}$ is close to zero or one. It is a measure of _node purity_ - a small value indicates that a node contains predominantly observations from a single class

* The _Entropy_:

$$ D = - \sum_{k=1}^K \hat{p}_{mk}log(\hat{p}_{mk})$$
similarly to the Gini, will take on a value near zero if all $\hat{p}_{mk}$ are near zero or one. Both measurements are quite similar numerically
  
* generally, classification error rate is preferable if prediction accuracy is the goal of the final pruned tree

### Trees vs Linear Models

* if relationship between features and response is well approximated by a linear model, then a linear regression would outperform trees
* if there is a highly non-linear and complex relationship between features and response, then trees may outperform linear regression

### Pros of Trees

* very easy to explain
* may closely mirror human decision-making more than regression and classification approaches in previous methods
* displayed graphiccaly and easily interpreted
* can handle qualitative predictors without the need to create dummy variables

### Cons of Trees

* generally don't have same level of predictive accuracy as other regression and classification techniques
* tend to be very non-robust, small change in data can cause large change in the tree

## Bagging, Random Forests, Boosting

### Bagging

* in order to solve the high variance problem in trees, _bagging_ can help reduce it through boostrapping procedures
  * recall in bootstrap, given $n$ independent observations $Z_{1},\ldots,Z_{n}$, each with variance $\sigma^2$
    * variance of mean $\overline{Z}$ is given by $\frac{\sigma^2}{n}$, thus has a lower variance
    * essentially we use repeated samples from our original training data to create  $B$ different training sets:
  
$$\hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{*b}(x)$$
which is called bagging

* in the context of regression trees, we construct $B$ different trees using $B$ boostrapped training sets, then avg the resulting predictions
  * avg out all the trees reduces the variance
* in classification trees, for a given test observation, we can record the class predicted by each of the $B$ tres, and take a _majority vote_, which is the overall prediction most commonly occurring among all the $B$ predictions
* important to note that using a large $B$ will not lead to overfitting
  * generally use $B=100$ to achieve sufficient performance

#### Out-of-Bag Error Estimation

* very straightforward way of estimating test error of a bagged model, without the need of CV
* on avg, each bagged tree makes use around two-thirds of observations (sampling with replacement of training set)
  * remaining one-third of observations not used are the _Out-of-Bag_ (OOB) observations
  * can predict response for $i$th observation using each of the trees in which that observation was OOB
    * yields around $B/3$ predictions for $i$th observation
  * with $B$ sufficiently large, OOB error is essentially the same as LOOCV
    * convenient when perform CV would be computationally infeasible

#### Variable Importance Measures  
* bagging improves prediction accuracy at the expense of interpretability
  * can obtain summary of important predictors using RSS or Gini index
    * for regression, record the total amount the RSS is decreased due to splits over a given predictor
    * for classification, sum the total amount that the Gini index is decreased by splits over a given predictor
    
### Random Forests

* improves over bagging through docorrelation of the trees
* similar to bagging, we build a number of trees based on bootstrapping, but now we choose a _random sample of m predictors_ as split candidates from the full set of $p$ predictors
  * split is only allowed to use one of those $m$ predictors
    * a fresh sample of $m$ predictors chosen at each split
  * typically choose $m \approx \sqrt{p}$
  * thus at each split, the algorithm is not even allowed to consider a majority of the available predictors
    * helps _decorrelate_ trees, to prevent the domination of one strong predictor on all the trees
  * when $m=p$, then the process is just bagging, hence bagging is a special case of random forest
  
### Boosting

* boosting works similarly to bagging, but now each tree is grown _sequentially_
  * uses info from previous tree
  * doesn't use bootstrap, instead tree is fit on a modified version of original data
  
**Algorithm for Boosting**

1. Set $\hat{f}(x)=0$ and $r_{i}=y_{i}$ for all $i$ in the training set

2. For $b = 1,2,\ldots,B$, repeat:
  * Fit a tree $\hat{f}^b$ with $d$ splits ($d+1$ terminal nodes) to training data $(X,r)$
  * Update $\hat{f}$ by adding shrunken version of new tree:
  
$$\hat{f}(x) \leftarrow \hat{f}(x) + \lambda\hat{f}^b(x)$$

  * Update the residuals:
  
$$ r_{i} \leftarrow r_{i} + \lambda\hat{f}^b(x)$$

3. Output the boosted model:

$$\hat{f}(x)=\sum_{b=1}^B \lambda\hat{f}^b(x)$$

* the idea is to slowly improve $\hat{f}$ in areas where it doesn't perform well
* shrinkage $\lambda$ slows the process down, allowing more and different shaped trees to attack residuals

Boosting has three tuning parameters:

1. Number of trees $B$. Unlike bagging and random forests, boosting can overfit if $B$ is too large, hence we use CV to choose B

2. shrinkage parameter $\lambda$, a small positive number and controls rate at which boosting learns (typical values are 0.01 or 0.001). Very small $\lambda$ can require large $B$ to achieve good performance

3. Number of splits $d$ in each tree. Controls complexity of boosted ensemble, and often $d=1$ works well, where each tree is a _stump_ of a single split. When $d=1$, boosted ensemble is fitting an additive model, since each term involves only one variable. Generally, $d$ is the _interaction depth_

* because growth of a tree depends on previous trees, smaller trees are typically sufficient
  * smaller trees can aid interpretability
  
# Applied

## Fitting Classification Trees
```{r}
library(tree)
library(ISLR)
attach(Carseats)
High <- ifelse(Sales <= 8, "No","Yes") #recode into binary variable
Carseats <- data.frame(Carseats, High)
```

```{r}
# fit classification tree to predict High using all variables but sales
tree.carseats <- tree(High~. -Sales, Carseats)
summary(tree.carseats)
```
summary function lists variables that are used as internal nodes in tree, the number of terminal nodes, and training error rate

Now we plot the tree:
```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```
Typing in the tree object prints out the split criterions, # of observations in the branch, deviance, the overall prediction for that branch, and the fraction of observations in that branch that take on values of Yes and No respectively.
```{r}
tree.carseats
```

Now we split train/test to evaluate test error:
```{r}
set.seed(1)
train <- sample(1:nrow(Carseats),200) # 50/50 split
Carseats.test <- Carseats[-train,]
High.test <- High[-train]
tree.carseats <- tree(High~. -Sales, Carseats,subset = train)
tree.pred <- predict(tree.carseats,Carseats.test,type = "class")
table(tree.pred,High.test)
```
correct predictions:
```{r}
(84+44)/200
```
So the prediction accuracy is around 64%

Next we consider if pruning helps our accuracy:
```{r}
set.seed(1)
cv.carseats <- cv.tree(tree.carseats,FUN = prune.misclass) # pruning our tree
names(cv.carseats)
cv.carseats
```
dev is the CV error rate, and thus it looks like the tree with 6 terminal nodes results in lowest error rate, with 53 CV-errors. k corresponds to the cost-complexity parameter used, in which this case was $\alpha$.

```{r}
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev, type = "b")
plot(cv.carseats$k,cv.carseats$dev,type = "b")
```

Now apply prune.misclass() to prune tree to obrain the 6 node tree:
```{r}
prune.carseats <- prune.misclass(tree.carseats, best=6)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```
Now lets test it on the test data:
```{r}
tree.pred <- predict(prune.carseats,Carseats.test, type = "class")
table(tree.pred,High.test)
```
```{r}
(86+49)/200
```
Seems like we have improved on the original tree by about 4%. Not only is it more accurate, but much more interpretable as well.

## Fitting Regression Trees

Let's create a train/test split on the Boston data set
```{r}
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2) # 50/50 split
tree.boston <- tree(medv~., Boston, subset = train)
summary(tree.boston)
```
In regression trees, deviance is the sum of squared errors for the tree

```{r}
plot(tree.boston)
text(tree.boston,pretty=0)
```
It seems that the most important variables are rm(avg number of rooms per dwelling), and lstat(% of lower status of population).

Let's prune the tree:
```{r}
set.seed(1)
cv.boston <- cv.tree(tree.boston)
cv.boston
plot(cv.boston$size,cv.boston$dev, type="b")
```
Using CV, seems like the one one with the best performance is the one with 6 terminal nodes

Now let's prune the tree:
```{r}
prune.boston <- prune.tree(tree.boston,best = 6)
plot(prune.boston)
text(prune.boston,pretty=0)
```
Let's now make predictions on test set:
```{r}
yhat <- predict(prune.boston,newdata = Boston[-train,])
boston.test <- Boston[-train,"medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat - boston.test)^2)
```
So the test set MSE is around 35.

## Baggin and Random Forests

```{r}
library(randomForest)
set.seed(1)

# Bagging
bag.boston <- randomForest(medv~., 
                           data = Boston, 
                           subset=train,
                           mtry = 13,
                           importance = T) # we set mtry = 13 since we are using all predictors

bag.boston
```
Let's see how well bagging performs on test set:
```{r}
yhat.bag <- predict(bag.boston,newdata = Boston[-train,])
plot(yhat.bag,boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```

MSE is around 23.6, which is significantly better than the optimally-pruned tree from before

Let's try using random forest with m = 6:
```{r}
set.seed(1)
rf.boston <- randomForest(medv~.,
                          data = Boston, 
                          subset = train,
                          mtry = 6,
                          importance = T)
yhat.rf <- predict(rf.boston, newdata = Boston[-train,])
mean((yhat.rf-boston.test)^2)
```
we can see that the test MSE has decreased down to 19.6, which is an improvement over bagging

We can see the importance of each variable using the importance() function:
```{r}
importance(rf.boston)
```
The left column %IncMSE is based on the mean decrease of accuracy in predictions on the OOB samples when given variable is excluded from the model. IncNodePurity is a measure of total decrease in node purity that results from splits over that variable, averaged over all trees. We can plot these below:

```{r}
varImpPlot(rf.boston)
```
We can see that rm and lstat are by far the two most important variables

## Boosting


```{r}
library(gbm)
set.seed(1)

boost.boston <- gbm(medv~.,
                    data = Boston[train,],
                    distribution = "gaussian",
                    n.trees = 5000,
                    interaction.depth = 4) # would use dist = bernoulli if it were classification

summary(boost.boston)
```
Again, we can see that rm and lstat are by far the most important variables

We can produce a _partial dependence plot_ for these two variables, which illustrate the marginal effect of the variables on the response after _integrating out_ the other variables:
```{r}
par(mfrow = c(2,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
```
We can see that house prices are increasing with rm and decreasing with lstat

Finally, we can use the boosted model to fit the test data:
```{r}
boost.boston <- gbm(medv~.,
                    data = Boston[train,],
                    distribution = "gaussian",
                    n.trees = 5000,
                    interaction.depth = 4,
                    verbose = F,
                    )

yhat.boost <- predict(boost.boston,
                      newdata = Boston[-train,],
                      n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```
The shrinkage default for gbm() is 0.1. We can see an incremental improvement of using boosting over random forests (improvement by about 1%).
