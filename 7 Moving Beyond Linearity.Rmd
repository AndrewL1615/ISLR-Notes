---
title: "7 Moving Beyond Linearity"
author: "Andrew Liang"
date: "11/20/2020"
output: pdf_document
---

# Notes

## Polynomial Regression
$$y_{i} = \beta_{0}+\beta_{1}x_{i} + \beta_{2}x^2_{i} + \ldots + \beta_{d}x^d_{i}+ \epsilon_{i}$$
up to degrees of $d$

* is a linear model with predictors $x_{i},x^2_{i},\ldots,x^d_{i}$
* usually $d$ is no greater than 3 or 4 otherwise model can become overly flexible

## Step Functions

* polynomial functions imposes a _global_ structure on the non-linear function of $X$
* use step functions to avoid imposing global strcuture
* break the range of X into _bins_, then fit different constant in each bin
  * converts continuous variable into an _ordered categorical variable_
  
Create cutpoints $c_{1},c_{2},\ldots,c_{K}$ in the range of X, then construct $K+1$ new variables:

$$C_{0}(X)=I(X<c_{1})$$
$$C_{0}(X)=I(c_{1}<X<c_{2})$$ 
$$C_{0}(X)=I(c_{1}<X<c_{2})$$ 
$$\ldots$$
$$C_{0}(X)=I(c_{K-1}<X<c_{K})$$ 
$$C_{0}(X)=I(c_{K}<X)$$ 
where $I(\cdot)$ is an _indicator function_ that returns a 1 if the condition is true, 0 otherwise.

For any value of $X$, $C_{0}(X) + C_{1}(X)+\ldots+C_{K}(X) = 1$, since $X$ must be in exactly one of the $K+1$ intervals

We can then use least sqaure to fit a linear model using $C_{1}(X),C_{2}(X),\ldots,C_{K}(X)$ as predictors:
$$y_{i}=\beta_{0} +\beta_{1}C_{1}(x_{i})+\beta_{2}C_{2}(x_{i})+\beta_{K}C_{K}(x_{i})+\epsilon_{i}$$

* thus for a given $X$, at most one of $C_{1},C_{2},\ldots,C_{K}$ can be non-zero

Unless there are natural breakpoints in predictors, piecewise-constant functions can miss the action

## Basis Functions

*polynomial and piecewise-constant regression are special cases of _basis function_ approach
  *idea is to have a family of functions or transformations that can be applied to variable $X$: $b_{1}(X),b_2{X},\ldots,b_{K}(X)$

Then fit the model:
$$ y_{i}=\beta_{o}+\beta_{1}b_{1}(x_{i})+\beta_{2}b_{2}(x_{i})+\ldots+\beta_{K}b_{K}(x_{i}) +\epsilon_{i}$$
* basis functions $b_{1}(\cdot),b_{2}(\cdot),\ldots,b_{K}(\cdot)$ must be fixed and known
* since this is just a standard linear model with predictors $b_{1}(x_{i}),\beta_{2}b_{2}(x_{i}),\ldots,b_{K}(x_{i})$, all inference tools for linear models are still avaiable in this setting

## Regression Splines

### Piecewise Polynomials

* similar to polynomial regressions, but instead of fitting it over the entire range of $X$, we fit separate polynomial regressions over different regions of $X$
* a quadratic polynomial with a single _knot_ at a point $c$ takes the form:

$$y_{i}=
\begin{cases}
\beta_{01}+\beta_{11}x^2_{i}+\beta_{21}x^2_{i}+\epsilon_{i} &\text{if \(x_{i} < c\)}\\
\beta_{02}+\beta_{12}x^2_{i}+\beta_{22}x^2_{i}+\epsilon_{i} &\text{if \(x_{i} \ge c\)}\\
\end{cases}$$

* obviously more knots lead to a more flexible piecewise polynomial
  * $K$ different knots through $X$ results in $K+1$ different polynomial regressions
* without constraints, the model will be discontinuous at each knot

### Constraints and Splines

* in order to fix discontinuity problem, we apply a constraints that the model must be continuous, or that both _first_ and _second_ derivatives are continuous
  * setting both first and second derivatives to be continuous allows for piecewise polynomials to be smooth
    * continuous AT the knot
    * decreases degree of freedom by 3 (continuity, continuity of first derivative, continuity of second derivative)
  * setting only the model to be continuous allows for the model to be continuous but not as smooth (sudden changes in direction at the knots)
    * discontinuous AT the knot
  * each constraint lowers degree of freedom

*Natural Cubic Splines*

* splines can have high varaince at the outer range of predictors (Where $X$ smaller than smallest knot, bigger than biggest knot)
  * to solve this, we add _boundary constraints_
    * enforce function to be linear at the boundary
      * will have lower CI at boundary regions

### Choosing Number of Knots
* common practice to place knots in a uniform fashion
  * specify desired degrees of freedom
* can see which produces best looking curve, or use CV
* regression splines often perform superior to polynomial regression
  * especially at the boundary regions, where variance is highly volatile
  
## Smoothing Splines

* to fit a smooth curve to data, want to find some function $g(x)$ so that:
$$RSS=\sum_{i=1}^n(y_{i}-g(x_{i}))^2$$

is minimized

* however, no constraints on $g(x)$ would allow us to choose $g$ such that it _interpolates_ all of the y_{i}, in other words, we can simply just overfit the data to the extreme
  * to solve this, we can add a penalty term and minimize:
  
$$ RSS=\sum_{i=1}^n(y_{i}-g(x_{i}))^2 + \lambda\int g''(t)^2dt$$
where $\lambda$ is a nonnegative _tuning parameter_
* want to minimize the integral of the second derivative of $g$ because
  * it is the measure of the total change in the function $g'(t)$ in the range of $t$
  * if *g* is smooth, then $g'(t)$ will be close to constant and $\int g''(t)^2dt$ will be small, vice versa
  * large $\lambda$ values will penalize jumpy functions and as $\lambda \rightarrow \infty$, $g$ will just be a straight line and thus perfectly smooth
    *$\lambda$ controls bias-variance tradeoff of smoothing spline
  * a function $g(x)$ that minimizes the above equation actually places knots at _every_ unique x values: $x_{1},x_{2},\ldots, x_{n}$!
    *NOT the same as a natural cubic spline, rather it is a _shrunken_ version of it, where $\lambda$ controls level of shrinkage
    
### Choosing Smoothing Parameter 

* seems like a smoothing spline might have far too many df, but $\lambda$ effectively controls roughness of spline, and hence controls the _effective degrees of freedom_
* selecting $\lambda$ is essentially equivalent to selecting how many df you want
* using LOOCV allows us to reduce RSS as small as possible:

$$RSS_{cv}(\lambda)=\sum_{i=1}^n (y_{i}-\hat{g}_{\lambda}^{(-i)}(x_{i}))^2$$

## Local Regression

* idea is to fit a function at a target point $x_{0}$ using only the nearby training observations
* Local Regression Algorithm at $X=x_{0}$: 

1) Gather fraction $s = k/n$ training points whose x_{i} are closest to x_{0}
2) Assign weight $K_{i0}=K(x_{i},x_{0})$ to each point in this neighborhood, so that point furthest from $x_{0}$ has weight 0, while closest has highest weight. All but $k$ nearest neighbors get weight 0
3) Fit _weighted least squares regression_ of $y_{i}$ on $x_{i}$ using aforementioned weights, by finding $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$ that minimize:
$$ \sum_{i=1}^nK_{i0}(y_{i}-\beta_{0}-\beta_{1}x_{i})^2 $$
4) Fitted value at $x_0$ is given by $\hat{f(x_{0})} = \hat{\beta_{0}} + \hat{\beta_{1}}x_{0}$

* the smaller value of $s$, the more local and wiggly our fit will be
* vice versa, the higher value of $s$ leads to a more global fit

## Generalized Additive Models (GAMs)

* extends standard linear model by allowing non-linear functions of each variable, while maintaining _additivity_
* can be applied to both quantitative/qualitative responses

### GAMs for Regression

* replace each linear component $\beta_{j}x_{ij}$ with a smooth, non-linear function $f_{j}(x_{ij})$:
$$ y_{i} = \beta_{0} + \sum_{j=1}^p f_{j}(x_{ij}) + \epsilon_{i}$$
* it is an _additive_ model because it calculates separate $f_{j}$ for each $X_{j}$, then ads together all of their contributions
* can use all of the aforementioned methods as building blocks to fit an additive model

#### Pros of GAMs
* allow us to fit non-linear $f_{j}$ to each $X_{j}$ where standard linear regressions will fail to capture
* potentially allow more accurate predictions for response $Y$
* since model is additive, we can examine each effect $X_{j}$ has on $Y$ individually holding all other variables fixed - useful for inference
* smoothness of $f_{j}$ for $X_{j}$ can be summarized via degrees of freedom

#### Cons of GAMs
* model restricted to be additive, thus interactions between variables can be missed
  * however we can manually add interaction terms or low-dimensional interaction functions $f_{jk}(X_{j},X_{k})$ to the model
  
### GAMs for Classification
$$log(\frac{p(X)}{1-p(X)})=\beta_{0} + \beta_{1}f_{1}(X_{1})+\cdots+\beta_{p}f_{p}(X_{p})$$

# Applied

```{r}
library(ISLR)
attach(Wage) #using Wage data
```

## Polynomial Regression and Step Functions

Fitting model of wage against age with degree 4
```{r}
fit <- lm(wage ~ poly(age,4), data = Wage)
coef(summary(fit))
```
Create a grid of values for age at which we want predictions:
```{r}
agelims <- range(age)
age.grid <- seq(from = agelims[1], to = agelims[2]) # list of age values
preds <- predict(fit, newdata = list(age = age.grid), se=T) # use fitted model to predict new age values
se.bands <- cbind(preds$fit + 2*preds$se.fit, preds$fit - 2 * preds$se.fit)
```

Plot the data and add fit from degree-4 polynomial:
```{r}
par(mfrow = c(1,1), mar = c(4.5,4.5,1,1), oma = c(0,0,4,0))
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Degree-4 Polynomial", outer=T)
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

Now we fit from linear to a degree-5 polynomial to seek the simplest model sufficient to explain relationship between wage and age:
```{r}
fit1 <- lm(wage ~ age, data = Wage)
fit2 <- lm(wage ~ poly(age, 2), data = Wage)
fit3 <- lm(wage ~ poly(age, 3), data = Wage)
fit4 <- lm(wage ~ poly(age, 4), data = Wage)
fit5 <- lm(wage ~ poly(age, 5), data = Wage)
anova(fit1, fit2, fit3, fit4, fit5)
```
Above is what is called a _nested_ models, where each model is nested in the proceeding models after

Looking at p-values, it seems that a cubic or quartic polynomial appear to be reasonable to fit the data, but lower or higher order models are not justified. Instead of ANOVA, CV involving the polynomial degree could be used as well


Now we explore a logistic approach; predicting whether an individual earns more than $250,000 per year
```{r}
#classification
fit <- glm(I(wage > 250) ~ poly(age, 4), data = Wage, family = binomial)
preds <- predict(fit, newdata = list(age = age.grid), se = T) #predict using fitted model using age values from grid
```

Creating confidence intervals for logit:
```{r}
pfit <- exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit <- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
se.bands <- exp(se.bands.logit)/(1+exp(se.bands.logit))
```

Plotting logit:
```{r}
plot(age, I(wage>250), xlim = agelims, type = "n", ylim = c(0,.2))
points(jitter(age), I((age>250)/5), cex = .5, pch = "l", col = "darkgrey")
lines(age.grid, pfit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

Fitting a step function:
```{r}
table(cut(age,4)) #split ages up into 4 groups
fit <- lm(wage ~ cut(age,4), data = Wage)
coef(summary(fit))
```
It seems that the age groups that are highly significant in predicing wage belong in the first two groups (ages 33.5~64.5)

## Splines
```{r}
library(splines)

fit <- lm(wage ~ bs(age, knots = c(25,40,60)), data = Wage)
pred <- predict(fit, newdata = list(age = age.grid), se = T)
plot(age,wage,col="gray")
lines(age.grid, pred$fit, lwd=2)
lines(age.grid,pred$fit + 2*pred$se, lty = "dashed")
lines(age.grid,pred$fit - 2*pred$se, lty = "dashed")

```

Above we specified knots at ages 25,50,60. Thus, six basis functions were used. From the above, a default of cubic splines are produced. (Cubic spline with three knots produces seven degrees of freedom: one intercept + 6 basis functions)

Could also use df option to produce splines at knots with uniform quantiles of the data:
```{r}
attr(bs(age,df=6),"knots")
```
Fitting a natural spline:
```{r}
fit2 <- lm(wage~ns(age,df=4),data=Wage)
pred2 <- predict(fit2,newdata=list(age=age.grid),se=T)

#plot
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
lines(age.grid, pred2$fit,col="red",lwd=2)
title("Smoothing Spline")
fit <- smooth.spline(age,wage,df=16) # function then determines which value of lambda leads to 16df
fit2 <- smooth.spline(age,wage,cv=T) # select smoothness by CV
print(fit2$df)
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"), col=c("red","blue"),lty=1,lwd=2,cex=.8)
```
Fitting a local regression:
```{r}
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Local Regression")
fit <- loess(wage~age, span=.2, data=Wage) # each neighborhood consists of 20% of data
fit2 <- loess(wage~age, span=.5, data = Wage) # 50% of data
lines(age.grid,predict(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)),col="blue",lwd=2)
legend("topright",legend=c("Span = 0.2","Span = 0.5"), col=c("red","blue"),lty=1,lwd=2,cex=.8)
```

## GAMs

```{r}
gam1 <- lm(wage~ns(year,4)+ns(age,5)+education, data = Wage)
```

```{r}
library(gam)

#now use smoothing splines from gam library
gam.m3 <- gam(wage~s(year,4)+s(age,5)+education, data = Wage)
```

Now we can plot it:
```{r}
par(mfrow=c(1,3))
plot(gam.m3, se=T,col="blue")
```
Using plot.Gam instead, on the model with natural splines:
```{r}
par(mfrow=c(1,3))
plot.Gam(gam1, se=T, col="red")
```

We can see that smoothing and natural splines project very similar results

Now we can use ANOVA testes to determine which of these three models is best:
```{r}
gam.m1 <- gam(wage~s(age,5) + education,data=Wage)
gam.m2 <- gam(wage~year+s(age,5) + education,data=Wage)

anova(gam.m1,gam.m2,gam.m3,test = "F")
```
Find that the GAM with a linear function of **year** is better than GAM with no **year** variable. But there is no evidence that a non-linear function of year is needed. This is reinforced by:

```{r}
summary(gam.m3)
```
Just like before, we can make predictions for gam objects:
```{r}
preds <- predict(gam.m2, newdata = Wage)
```

We can implement local regression fits as buildings blocks in GAM:
```{r}
gam.lo <- gam(wage~s(year,4)+lo(age,span=0.7)+education,data=Wage)

par(mfrow = c(1,3))
plot.Gam(gam.lo, se=T, col="green")
```

We cacn also use lo() to create interactions before calling gam():
```{r}
gam.lo.i <- gam(wage~lo(year,age,span=0.5)+education, data=Wage)

```
```{r}
library(akima)
par(mfrow = c(1,2))
plot(gam.lo.i)
```

For logistic regression GAMs:
```{r}
gam.lr <- gam(I(wage>250)~year+s(age,5)+education, family = binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T, col="green")
```
From the education graph, we can see that there are no high earners <HS grad:
```{r}
table(education,I(wage>250))
```
Thus, we fit a logistic regression GAM using all but this category:
```{r}
gam.lr.s <- gam(I(wage>250)~year+s(age,df=5)+education,family = binomial, data = Wage, subset = (education != "1. < HS Grad"))

par(mfrow=c(1,3))
plot(gam.lr.s,se=T,col = "green")
```

