---
title: "6 Linear Model Selection and Regularization"
author: "Andrew Liang"
date: "10/7/2020"
output: pdf_document
---

# Notes

## Subset Selection

### Best Subset

1. Fit $M_{0}$, the null model, with no predictors. (only predicts sample mean for each observation).
2. For $k = 1,2,\ldots,p:$ 
    + Fit all $\binom{p}{k}$ models that contain exactly k predictors
    + Choose the best among the $\binom{p}{k}$ models and call it $M_{k}$. Best is defined as having smallest RSS, or equivalently largest $R^2$
3. Select single best model among $M_{0},\ldots,M_{p}$ using CV prediction error, $C_{p} (AIC)$, BIC, or adjusted $R^2$

* Suffers from computational limitations, as the number of possible models grows rapidly as $p$ increases ($2^p$ models)

### Forward Stepwise Selection

1. Fit $M_{0}$, the null model, with no predictors.
2. For $k=0,\ldots,p-1:$
    + Consider all $p-k$ models that augment the predictors in $M_{k}$ with one additional predictor
    + Choose best among $p-k$ models ($M_{k+1}$)
  
3. Select single best model among $M_{0},\ldots,M_{p}$ using CV prediction error, $C_{p} (AIC)$, BIC, or adjusted $R^2$

* Much less computationally expensive compared to best subset
* However, not guaranteed to find best subset model
* Can be applied in high-dimensional setting ($n<p$)

### Backward Stepwise Selection

1. Fit $M_{p}$, the full model, with all predictors.
2. For $k=p,p-1,\ldots,1:$
    + Consider all $k$ models that contain all but one of the predictors in $M_{k}$, for a total of $k-1$ predictors
    + Choose best among $k$ models ($M_{k-1}$)
  
3. Select single best model among $M_{0},\ldots,M_{p}$ using CV prediction error, $C_{p} (AIC)$, BIC, or adjusted $R^2$

* Also not guaranteed to find best model
* REQUIRES that $n$ is larger than $p$

Best subset, forward, and backward selection generally give similar but not identical models

## Choosing the Optimal Model

Techniques for adjusting the training error for the model size are available

1. $C_{p}$
    + for a fitted least squares model containing $d$ predictors and the variance of the error $\hat{\sigma}^2$, $C_{p}$ estiamte of test MSE is:
    $$C_{p} = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)$$
    
    + penalty increases as number of predictors in model increases
    + choose model with lowest $C_{p}$ value
    
2. AIC
    + defined for models fit by maximum likelihood (least squares)
    $$AIC = \frac{1}{n\hat{\sigma}^2}(RSS + 2d\hat{\sigma}^2)$$
    + proportional to $C_{p}$
    
3. BIC (similar to $C_{p}$ and AIC, but from a Bayesian POV)
    $$BIC = \frac{1}{n\hat{\sigma}^2}(RSS + 2log(n)d\hat{\sigma}^2)$$ 
    + replaces $2d\hat{\sigma}^2$ with $log(n)d\hat{\sigma}^2$
    + since $log(n) > 2$ for any $n>7$, BIC generally places heavier penalty on models with many predictors 
  
4. Adjusted $R^2$
    
    $$Adjusted R^2 = 1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$$
    
    + unlike previous penalties, we want to choose model with highest adjusted $R^2$
    + despite popularity, is not as statistically motivated as the previous penalties
    
## Shrinkage Methods

* fit model using all predictors and regularizes coefficients/shrinks coefficients towards zero
  + reduces variance
  
### Ridge Regression

wants to minimize:
$$ \sum_{i=1}^{n} (y_{i}-\beta_{0} - \sum_{j=1}^{p} \beta_{j}x_{ij})^2 + \lambda\sum_{j=1}^p \beta_{j}^2 = RSS + \lambda\sum_{j=1}^p \beta_{j}^2$$

* $\lambda\sum_{j=1}^n \beta_{j}^2$ is the shrinkage penalty
* $\lambda \ge 0$ is the tuning parameter
  + as $\lambda\to\infty$, the model coefficients approaches zero (except for model intercept $\beta_{0}$)
* selecting $\lambda$ value is important (can use CV)
* best to apply ridge after predictors have been standardized (due to potential scaling issues):
$$\tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{(\frac{1}{n}\sum_{i=1}^{n} (x_{ij} - \overline{x}_{j})^2)}}$$

* important to note that all the predictors will still be included in the model; only the magnitude of the coefficients is affected

### The Lasso

* similar to ridge, but has the ability to exclude predictors in final model (better for interpretability)

wants to minimize:
$$ \sum_{i=1}^{n} (y_{i}-\beta_{0} - \sum_{j=1}^{p} \beta_{j}x_{ij})^2 + \lambda\sum_{j=1}^p |\beta_{j}| = RSS + \lambda\sum_{j=1}^p |\beta_{j}|$$

* $\lambda$ penalty has the effect of forcing some of the coefficient estimates to be zero when $\lambda$ is sufficiently large

### Ridge vs Lasso
* generally, ridge performs better when response is a function of many predictors, with all coefficients roughly the same size
* generally, lasso performs better when only a relatively small number of predictors have substantial coefficients, and remaining variables are very small coefficients
* both perform shrinkage, whereas ridge shrinks the coefficients by the same proportion, whereas lasso shrinks all coefficients toward 0 by the same amount, and sufficiently small coefficients are shrunken all the way to 0

## Dimension Reduction Methods

* idea is to transform the predictors then fit a least squares model

let $Z_{1},Z_{2},\cdots,Z_{M}$ represent $M<p$ linear combinations of original $p$ predictors:

$$Z_{M} = \sum_{j=1}^{p} \phi_{jm}X_{j}$$
for some constants $\phi_{1m},\phi_{2m},\cdots,\phi_{pm}$, then we fit the linear regression model:
$$y_{i} = \theta_{0} + \sum_{m=1}^{M} \theta_{m}z_{im} + \epsilon_{i}$$
* dimension of the problem has been reduced from $p+1$ to $M+1$
* can often outperform least squares IF the choice of $Z_{1},Z_{2},\cdots,Z_{M}$ is chosen wisely

### Principal Components Analysis (PCA)

* dimension reduction technique in which the _first principle component_ direction of the data is that along which the observations _vary the most_ (have highest variance)
  + is a vector that defines a line that minimizes perpendicular distances between each point and the line (distance represents the projection of the point onto that line)
* PCA scores for the 1st component is defined as:

$$Z_{j1} = \sum_{j=1}^{p} \beta_{j}(X_{j}-\overline{X}_{j})$$
* can calculate up to $p$ distinct pcincipal components
* 2nd PC is a linear combination of variables that is uncorrelated with $Z_{1}$, or equivalently must be perpendicular/orthogonal to $Z_{1}$
* first component will always contain the most info

#### Principal Components Regression Approach (PCR)

* involved using $Z_{1},Z_{2},\cdots,Z_{M}$ as predictors in linear regression
* assume that the directions in which $X_{1},\ldots,X_{p}$ _show the most variation are the directions that are associated with_ $Y$
* will be better than the original linear model with $X_{1},\ldots,X_{p}$ as predictors if PCR assumptions are met
* performs better when the first few principal components are sufficient to capture most of variation in the predictors and their relationships with the response
* since PCR is a lienar combination of all p of the _original_ features, it is not a feature selection method
* number of components $M$ usually chosen by CV
* usually recommended to standardize predictors using method from ridge if these predictors aren't on the same scale
* example of an _unsupervised_ method

### Partial Least Squares (PLS)

* a supervised method similar to PCA where it is dimension reduction
* same process as PCR, but also uses response $Y$ to find directions that help explain both response and predictors
  + places highest weight on variables strongly correlated with $Y$
* often performs no better than PCR or ridge

## Considerations in High Dimensional Data
* when $p \ge n$, linear regression/logistic regression should not be performed
* $C_{p},AIC,BIC$ unfortunately are not appropriate in high dimensional settings, as estimating $\hat{\sigma}^2$ is problematic
* 3 important points:
    1. regularization/shrinkage is very important in high-dimensional settings
    2. appropriate tuning parameter selection key for good predictive performance
    3. test error tends to increase as dimensionality increases, unless the additional predictors are truly associated with response
* adding new features is a truly a double-edged sword, depending whether or not they are truly associated with $Y$
* should _never_ use sum of squared errors, p-values, $R^2$ statistics as evidence of model fit in high dimensional setting

# Applied

