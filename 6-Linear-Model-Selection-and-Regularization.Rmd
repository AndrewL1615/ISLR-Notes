---
title: "6 Linear Model Selection and Regularization"
author: "Andrew Liang"
date: "10/7/2020"
output: pdf_document
---

# Notes

## Subset Selection

### Best Subset

1. Fit $M_{0}$, the null model, with no predictors. (only predicts sample mean for each observation).
2. For $k = 1,2,\ldots,p:$ 
    + Fit all $\binom{p}{k}$ models that contain exactly k predictors
    + Choose the best among the $\binom{p}{k}$ models and call it $M_{k}$. Best is defined as having smallest RSS, or equivalently largest $R^2$
3. Select single best model among $M_{0},\ldots,M_{p}$ using CV prediction error, $C_{p} (AIC)$, BIC, or adjusted $R^2$

* Suffers from computational limitations, as the number of possible models grows rapidly as $p$ increases ($2^p$ models)

### Forward Stepwise Selection

1. Fit $M_{0}$, the null model, with no predictors.
2. For $k=0,\ldots,p-1:$
    + Consider all $p-k$ models that augment the predictors in $M_{k}$ with one additional predictor
    + Choose best among $p-k$ models ($M_{k+1}$)
  
3. Select single best model among $M_{0},\ldots,M_{p}$ using CV prediction error, $C_{p} (AIC)$, BIC, or adjusted $R^2$

* Much less computationally expensive compared to best subset
* However, not guaranteed to find best subset model
* Can be applied in high-dimensional setting ($n<p$)

### Backward Stepwise Selection

1. Fit $M_{p}$, the full model, with all predictors.
2. For $k=p,p-1,\ldots,1:$
    + Consider all $k$ models that contain all but one of the predictors in $M_{k}$, for a total of $k-1$ predictors
    + Choose best among $k$ models ($M_{k-1}$)
  
3. Select single best model among $M_{0},\ldots,M_{p}$ using CV prediction error, $C_{p} (AIC)$, BIC, or adjusted $R^2$

* Also not guaranteed to find best model
* REQUIRES that $n$ is larger than $p$

Best subset, forward, and backward selection generally give similar but not identical models

## Choosing the Optimal Model

Techniques for adjusting the training error for the model size are available

1. $C_{p}$
    + for a fitted least squares model containing $d$ predictors and the variance of the error $\hat{\sigma}^2$, $C_{p}$ estiamte of test MSE is:
    $$C_{p} = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)$$
    
    + penalty increases as number of predictors in model increases
    + choose model with lowest $C_{p}$ value
    
2. AIC
    + defined for models fit by maximum likelihood (least squares)
    $$AIC = \frac{1}{n\hat{\sigma}^2}(RSS + 2d\hat{\sigma}^2)$$
    + proportional to $C_{p}$
    
3. BIC (similar to $C_{p}$ and AIC, but from a Bayesian POV)
    $$BIC = \frac{1}{n\hat{\sigma}^2}(RSS + 2log(n)d\hat{\sigma}^2)$$ 
    + replaces $2d\hat{\sigma}^2$ with $log(n)d\hat{\sigma}^2$
    + since $log(n) > 2$ for any $n>7$, BIC generally places heavier penalty on models with many predictors 
  
4. Adjusted $R^2$
    
    $$Adjusted R^2 = 1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$$
    
    + unlike previous penalties, we want to choose model with highest adjusted $R^2$
    + despite popularity, is not as statistically motivated as the previous penalties
    
## Shrinkage Methods

* fit model using all predictors and regularizes coefficients/shrinks coefficients towards zero
  + reduces variance
  
### Ridge Regression

wants to minimize:
$$ \sum_{i=1}^{n} (y_{i}-\beta_{0} - \sum_{j=1}^{p} \beta_{j}x_{ij})^2 + \lambda\sum_{j=1}^p \beta_{j}^2 = RSS + \lambda\sum_{j=1}^p \beta_{j}^2$$

* $\lambda\sum_{j=1}^n \beta_{j}^2$ is the shrinkage penalty
* $\lambda \ge 0$ is the tuning parameter
  + as $\lambda\to\infty$, the model coefficients approaches zero (except for model intercept $\beta_{0}$)
* selecting $\lambda$ value is important (can use CV)
* best to apply ridge after predictors have been standardized (due to potential scaling issues):
$$\tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{(\frac{1}{n}\sum_{i=1}^{n} (x_{ij} - \overline{x}_{j})^2)}}$$

* important to note that all the predictors will still be included in the model; only the magnitude of the coefficients is affected

### The Lasso

* similar to ridge, but has the ability to exclude predictors in final model (better for interpretability)

wants to minimize:
$$ \sum_{i=1}^{n} (y_{i}-\beta_{0} - \sum_{j=1}^{p} \beta_{j}x_{ij})^2 + \lambda\sum_{j=1}^p |\beta_{j}| = RSS + \lambda\sum_{j=1}^p |\beta_{j}|$$

* $\lambda$ penalty has the effect of forcing some of the coefficient estimates to be zero when $\lambda$ is sufficiently large

### Ridge vs Lasso
* generally, ridge performs better when response is a function of many predictors, with all coefficients roughly the same size
* generally, lasso performs better when only a relatively small number of predictors have substantial coefficients, and remaining variables are very small coefficients
* both perform shrinkage, whereas ridge shrinks the coefficients by the same proportion, whereas lasso shrinks all coefficients toward 0 by the same amount, and sufficiently small coefficients are shrunken all the way to 0

## Dimension Reduction Methods

* idea is to transform the predictors then fit a least squares model

let $Z_{1},Z_{2},\cdots,Z_{M}$ represent $M<p$ linear combinations of original $p$ predictors:

$$Z_{M} = \sum_{j=1}^{p} \phi_{jm}X_{j}$$
for some constants $\phi_{1m},\phi_{2m},\cdots,\phi_{pm}$, then we fit the linear regression model:
$$y_{i} = \theta_{0} + \sum_{m=1}^{M} \theta_{m}z_{im} + \epsilon_{i}$$
* dimension of the problem has been reduced from $p+1$ to $M+1$
* can often outperform least squares IF the choice of $Z_{1},Z_{2},\cdots,Z_{M}$ is chosen wisely

### Principal Components Analysis (PCA)

* dimension reduction technique in which the _first principle component_ direction of the data is that along which the observations _vary the most_ (have highest variance)
  + is a vector that defines a line that minimizes perpendicular distances between each point and the line (distance represents the projection of the point onto that line)
* PCA scores for the 1st component is defined as:

$$Z_{j1} = \sum_{j=1}^{p} \beta_{j}(X_{j}-\overline{X}_{j})$$
* can calculate up to $p$ distinct pcincipal components
* 2nd PC is a linear combination of variables that is uncorrelated with $Z_{1}$, or equivalently must be perpendicular/orthogonal to $Z_{1}$
* first component will always contain the most info

#### Principal Components Regression Approach (PCR)

* involved using $Z_{1},Z_{2},\cdots,Z_{M}$ as predictors in linear regression
* assume that the directions in which $X_{1},\ldots,X_{p}$ _show the most variation are the directions that are associated with_ $Y$
* will be better than the original linear model with $X_{1},\ldots,X_{p}$ as predictors if PCR assumptions are met
* performs better when the first few principal components are sufficient to capture most of variation in the predictors and their relationships with the response
* since PCR is a lienar combination of all p of the _original_ features, it is not a feature selection method
* number of components $M$ usually chosen by CV
* usually recommended to standardize predictors using method from ridge if these predictors aren't on the same scale
* example of an _unsupervised_ method

### Partial Least Squares (PLS)

* a supervised method similar to PCA where it is dimension reduction
* same process as PCR, but also uses response $Y$ to find directions that help explain both response and predictors
  + places highest weight on variables strongly correlated with $Y$
* often performs no better than PCR or ridge

## Considerations in High Dimensional Data
* when $p \ge n$, linear regression/logistic regression should not be performed
* $C_{p},AIC,BIC$ unfortunately are not appropriate in high dimensional settings, as estimating $\hat{\sigma}^2$ is problematic
* 3 important points:
    1. regularization/shrinkage is very important in high-dimensional settings
    2. appropriate tuning parameter selection key for good predictive performance
    3. test error tends to increase as dimensionality increases, unless the additional predictors are truly associated with response
* adding new features is a truly a double-edged sword, depending whether or not they are truly associated with $Y$
* should _never_ use sum of squared errors, p-values, $R^2$ statistics as evidence of model fit in high dimensional setting

# Applied

## Subset Selection

```{r}
library(ISLR)
names(Hitters)
summary(Hitters)

Hitters <- na.omit(Hitters) #omit na rows

library(leaps) #for subset selection

# best subset function
regfit.full <- regsubsets(Salary~., data = Hitters) #default up to 8 variables
summary(regfit.full)

regfit.full <- regsubsets(Salary~., data = Hitters, nvmax = 19) #set max # of variables to 19
reg.summary <- summary(regfit.full)
reg.summary$names #list of accuracy/penalty measurements

par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "p")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type ="p")
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type ="p")
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type ="p")

#seems that the number of variables that best fit the model is around 10

par(mfrow = c(1,1))
plot(regfit.full, scale = "Cp") #shows Cp values for all combinations
coef(regfit.full, 10) #coefficients for the 10 variables in model

#10 variables include AtBat, Hits, Walks, CAtBat, CRuns, CRBI, CWalks, DivisionW, PutOuts, Assists
```

```{r}
#Forward and Backward Selection

regfit.fwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)

regfit.bwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)

#comparing forward and backward
par(mfrow = c(1,2))
plot(regfit.fwd, scale = "Cp")
plot(regfit.bwd, scale = "Cp")

#in 7 variable model, selected variables are different
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)

#choosing models using Validation Set and CV
set.seed(1)
train <- sample(c(TRUE,FALSE), nrow(Hitters), replace = TRUE)
test <- !train

#perform best subset on train 
regfit.best <- regsubsets(Salary ~., data = Hitters[train,], nvmax = 19)

test.mat <- model.matrix(Salary ~ ., data = Hitters[test,])
val.errors <- rep(NA,19)

for(i in 1:19){
  coefi <- coef(regfit.best, id = i) #extract coefficients from regfit.best for each model of size i
  pred <- test.mat[,names(coefi)] %*% coefi #gives us the predicted value for each observation
  val.errors[i] <- mean((Hitters$Salary[test] - pred)^2) #MSE for each model of size i
}

val.errors
which.min(val.errors) #7 variables gives us the lowest test MSE
coef(regfit.best,7)

#function for predicting subset selection
predict.regsubsets = function(object, newdata, id, ...) {
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  mat[, names(coefi)] %*% coefi
}
```

```{r}
#Using k-fold validations

k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters),replace = T) #sample numbers 1 to 10 of length = dataset
table(folds)
cv.errors <- matrix(NA,k,19, dimnames = list(NULL, paste(1:19))) #matrix to store results

for(j in 1:k){
  best.fit <- regsubsets(Salary~., data = Hitters[folds != j,], nvmax = 19)
  for(i in 1:19){
    pred <- predict(best.fit, Hitters[folds == j,], id = i)
    #(i,j)th element corresponds to test MSE for ith CV for the best j-variable model
    cv.errors[j,i] <- mean((Hitters$Salary[folds == j] - pred)^2)
  }
}

mean.cv.errors <- apply(cv.errors,2,mean) #get a vector of the avg jth validation error for the jth model
mean.cv.errors
par(mfrow = c(1,1))
plot(mean.cv.errors, type = "b") #selects a 10 variable model

#perform best subset with 10 variables
reg.best <- regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(reg.best,10)
```

## Ridge and Lasso

```{r}
#Ridge Regression

x <- model.matrix(Salary~., data = Hitters)[,-1] #create matrix of values for all predictors
#also transforms qualitative variables into dummy variables
y <- Hitters$Salary
```

```{r}
library(glmnet)

grid <- 10^seq(10,-2, length = 100) #lambda values from 10^10 to 10^-2
ridge.mod <- glmnet(x,y,alpha = 0, lambda = grid) #alpha = 0 for ridge, 1 for lasso
ridge.mod$lambda[50]
coef(ridge.mod)[,50] #Ridge coefficients for lambda = 11498
ridge.mod$lambda[60]
coef(ridge.mod)[,60] #Ridge coefficients for lambda = 705
predict(ridge.mod, s = 50, type = "coefficients")[1:20,] #predict coef for lambda = 50

plot(ridge.mod, xvar = "lambda", label = T) #plot of coefficients against lambda values
```

```{r}
#split train/test
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

#fit ridge model on train
ridge.mod <- glmnet(x[train,],y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2) #evaluate test MSE with lambda = 4
```
MSE is 142199

```{r}
ridge.pred <- predict(ridge.mod, s=0, newx=x[test,]) #fitting ridge with lambda = 0
mean((ridge.pred - y.test)^2)
lm(y~x, subset = train)
predict(ridge.mod,s=0, type="coefficients")[1:20,] #comparing ridge to original linear model
```

```{r}
set.seed(1)
cv.ridge <- cv.glmnet(x[train,],y[train], alpha = 0) #CV with default 10 folds
plot(cv.ridge)
bestlam <- cv.ridge$lambda.min
bestlam
```

Seems like the lambda with the lowest test MSE is 326

```{r}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
testMSEridge <- mean((ridge.pred - y.test)^2)
```
The test MSE is 139856

Refitting ridge regression model using lambda chosen by CV
```{r}
out <- glmnet(x,y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20]
```
```{r}
#Lasso
lasso.mod <- glmnet(x[train,],y[train],alpha = 1)
plot(lasso.mod)
```

Same process as before, using alpha = 1. 
```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```
Seems like the best lambda value is around 9

```{r}
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
testMSElasso <- mean((lasso.pred - y.test)^2)
```
test MSE for CV lambda is 143668

```{r}
out <- glmnet(x,y, alpha = 1,lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20,]
lasso.coef
```
We can see that some variables have been shrunken down to 0. In this case, it seems that test MSE actually performed better with the Ridge than Lasso, but the Lasso model is notably more sparse, making it easier for interpretation

##PCR and PLS

```{r}
library(pls)
set.seed(1)
pcr.fit <- pcr(Salary~., data = Hitters, scale = T, validation = "CV")
#scale standardizes each predictor, validation = CV computes 10-fold CV
summary(pcr.fit)
```

```{r}
#validation plot with CV MSE
validationplot(pcr.fit, val.type = "MSEP")
```

Seems that MSE is lowest at 16, but CV error is roughly the same at 1 PC score

```{r}
#train/test
set.seed(1)
pcr.fit <- pcr(Salary~., data = Hitters, subset = train, scale = T,
               validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
```

lowest CV error occurs when m = 5, so we now compute the test MSE
```{r}
pcr.pred <- predict(pcr.fit, x[test,], ncomp = 5)
testMSEpcr <- mean((pcr.pred - y.test)^2)
```
Seems like the test MSE is competitive with Lasso and Ridge

```{r}
#PLS
set.seed(1)
pls.fit <- plsr(Salary~., data = Hitters, subset = train, scale = T,
                validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
```
Lowest CV error seems to be M = 1

```{r}
pls.pred <- predict(pls.fit,x[test,],ncomp = 1)
testMSEpls <- mean((pls.pred - y.test)^2)
```
Seems like the MSE is a little higher than the other methods

```{r}
pls.fit <- plsr(Salary~., data = Hitters, scale = T,
                ncomp = 1)
summary(pls.fit)
```

Box graph of all the test MSEs using the different methods
```{r}
library(ggplot2)
alltestMSE <- c(testMSEridge, testMSElasso, testMSEpcr, testMSEpls)
barplot(alltestMSE, 
        names.arg = c("Ridge,Lasso,PCR,PLS"),
        cex.names = 0.8,
        args.legend = alltestMSE,
        xlab = "Type of Regularization", 
        ylab = "Test MSE")
```